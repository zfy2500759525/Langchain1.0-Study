# 01 - Hello LangChain: ç¬¬ä¸€ä¸ª LLM è°ƒç”¨

## å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ¨¡å—ï¼Œä½ å°†å­¦ä¹ ï¼š

1. **LangChain 1.0 çš„æ ¸å¿ƒæ¦‚å¿µ**
   - LangChain 1.0 æ„å»ºåœ¨ LangGraph è¿è¡Œæ—¶ä¹‹ä¸Š
   - ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–æ¥å£
   - ç®€åŒ–çš„ API è®¾è®¡

2. **init_chat_model å‡½æ•°**
   - å¦‚ä½•åˆå§‹åŒ–èŠå¤©æ¨¡å‹
   - æ”¯æŒçš„å‚æ•°å’Œé…ç½®é€‰é¡¹
   - è·¨æ¨¡å‹æä¾›å•†çš„ç»Ÿä¸€æ¥å£

3. **invoke æ–¹æ³•**
   - åŒæ­¥è°ƒç”¨æ¨¡å‹
   - è¾“å…¥æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ã€æ¶ˆæ¯åˆ—è¡¨ã€å­—å…¸ï¼‰
   - è¿”å›å€¼ç»“æ„

4. **Messagesï¼ˆæ¶ˆæ¯ç±»å‹ï¼‰**
   - SystemMessageï¼šç³»ç»Ÿæç¤º
   - HumanMessageï¼šç”¨æˆ·è¾“å…¥
   - AIMessageï¼šAI å“åº”

---

## æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

### 1. init_chat_model - æ¨¡å‹åˆå§‹åŒ–

`init_chat_model` æ˜¯ LangChain 1.0 ä¸­ç”¨äºåˆå§‹åŒ–èŠå¤©æ¨¡å‹çš„**ç»Ÿä¸€æ¥å£**ã€‚

#### åŸºæœ¬è¯­æ³•

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "provider:model_name",  # æä¾›å•†:æ¨¡å‹åç§°
    api_key="your-api-key",  # API å¯†é’¥ï¼ˆå¯é€‰ï¼Œå¯ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    temperature=0.7,         # æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
    max_tokens=1000,         # æœ€å¤§ token æ•°ï¼ˆå¯é€‰ï¼‰
    **kwargs                 # å…¶ä»–æ¨¡å‹ç‰¹å®šå‚æ•°
)
```

#### å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `model` | `str` | **å¿…éœ€**ã€‚æ ¼å¼ä¸º `"provider:model_name"`ï¼Œå¦‚ `"groq:llama-3.3-70b-versatile"` | æ—  |
| `api_key` | `str` | API å¯†é’¥ã€‚å¦‚æœä¸æä¾›ï¼Œä¼šä»ç¯å¢ƒå˜é‡ä¸­è¯»å–ï¼ˆå¦‚ `GROQ_API_KEY`ï¼‰ | `None` |
| `temperature` | `float` | æ§åˆ¶è¾“å‡ºéšæœºæ€§ã€‚èŒƒå›´ 0.0-2.0ã€‚<br>- `0.0`ï¼šæœ€ç¡®å®šæ€§<br>- `1.0`ï¼šé»˜è®¤ï¼Œå¹³è¡¡<br>- `2.0`ï¼šæœ€éšæœº | `1.0` |
| `max_tokens` | `int` | é™åˆ¶æ¨¡å‹è¾“å‡ºçš„æœ€å¤§ token æ•°é‡ | æ¨¡å‹é»˜è®¤å€¼ |
| `model_kwargs` | `dict` | ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„é¢å¤–å‚æ•° | `{}` |

#### æ”¯æŒçš„æä¾›å•†æ ¼å¼

```python
# Groq
"groq:llama-3.3-70b-versatile"
"groq:mixtral-8x7b-32768"
"groq:gemma2-9b-it"

# OpenAI
"openai:gpt-4"
"openai:gpt-3.5-turbo"

# Anthropic
"anthropic:claude-sonnet-4-5-20250929"

# å…¶ä»–æä¾›å•†...
```

#### ä¸ºä»€ä¹ˆä½¿ç”¨ init_chat_modelï¼Ÿ

1. **ç»Ÿä¸€æ¥å£**ï¼šæ— éœ€è®°ä½æ¯ä¸ªæä¾›å•†çš„ä¸åŒåˆå§‹åŒ–æ–¹å¼
2. **æ˜“äºåˆ‡æ¢**ï¼šåªéœ€ä¿®æ”¹æ¨¡å‹å­—ç¬¦ä¸²å³å¯åˆ‡æ¢æ¨¡å‹
3. **ç±»å‹å®‰å…¨**ï¼šæä¾›æ›´å¥½çš„ç±»å‹æç¤º
4. **ç®€æ´æ˜äº†**ï¼šå‡å°‘æ ·æ¿ä»£ç 

#### ç¤ºä¾‹

```python
from langchain.chat_models import init_chat_model
import os

# æ–¹å¼ 1ï¼šç›´æ¥ä¼ é€’ API key
model = init_chat_model(
    "groq:llama-3.3-70b-versatile",
    api_key="your-groq-api-key"
)

# æ–¹å¼ 2ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆæ¨èï¼‰
model = init_chat_model(
    "groq:llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY")
)

# æ–¹å¼ 3ï¼šé…ç½®æ¸©åº¦å’Œ token é™åˆ¶
model = init_chat_model(
    "groq:llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY"),
    temperature=0.0,    # æœ€ç¡®å®šæ€§è¾“å‡º
    max_tokens=500      # é™åˆ¶è¾“å‡ºé•¿åº¦
)
```

---

### 2. invoke æ–¹æ³• - è°ƒç”¨æ¨¡å‹ï¼ˆæ·±å…¥è¯¦è§£ï¼‰

`invoke` æ˜¯ LangChain ä¸­**æœ€æ ¸å¿ƒçš„æ–¹æ³•**ï¼Œç”¨äºåŒæ­¥è°ƒç”¨ LLM æ¨¡å‹ã€‚ç†è§£ invoke æ˜¯å­¦ä¹  LangChain çš„å…³é”®ã€‚

---

#### ğŸ¯ invoke æ–¹æ³•åšä»€ä¹ˆï¼Ÿ

ç®€å•æ¥è¯´ï¼Œ`invoke` æ–¹æ³•çš„ä½œç”¨å°±æ˜¯ï¼š

1. **æ¥æ”¶ä½ çš„è¾“å…¥**ï¼ˆé—®é¢˜ã€æŒ‡ä»¤ã€å¯¹è¯å†å²ç­‰ï¼‰
2. **å‘é€ç»™ LLM æ¨¡å‹**ï¼ˆå¦‚ GPT-4, Llama, Claude ç­‰ï¼‰
3. **è¿”å›æ¨¡å‹çš„å“åº”**ï¼ˆæ–‡æœ¬å›å¤ + å…ƒæ•°æ®ä¿¡æ¯ï¼‰

**æµç¨‹å›¾ï¼š**
```
ä½ çš„è¾“å…¥ â†’ invoke() â†’ LLM æ¨¡å‹ â†’ å“åº” â†’ è¿”å›ç»™ä½ 
```

---

#### ğŸ“ åŸºæœ¬è¯­æ³•

```python
response = model.invoke(input, config=None)
```

**å‚æ•°è¯¦è§£ï¼š**

| å‚æ•° | ç±»å‹ | è¯´æ˜ | å¿…éœ€ | é»˜è®¤å€¼ |
|------|------|------|------|--------|
| `input` | `str` \| `list[dict]` \| `list[Message]` | ä½ è¦å‘é€ç»™æ¨¡å‹çš„å†…å®¹ | âœ… å¿…éœ€ | æ—  |
| `config` | `dict` | é«˜çº§é…ç½®ï¼ˆå›è°ƒå‡½æ•°ã€å…ƒæ•°æ®ã€æ ‡ç­¾ç­‰ï¼‰ | âŒ å¯é€‰ | `None` |

---

#### ğŸ” æ·±å…¥ç†è§£ input å‚æ•° - ä¸‰ç§è¾“å…¥æ ¼å¼

è¿™æ˜¯æœ€å®¹æ˜“å›°æƒ‘çš„åœ°æ–¹ï¼`invoke` æ”¯æŒ**ä¸‰ç§ä¸åŒçš„è¾“å…¥æ ¼å¼**ï¼Œè®©æˆ‘ä»¬é€ä¸€è¯¦è§£ï¼š

---

##### ğŸ“Œ æ ¼å¼ 1ï¼šçº¯å­—ç¬¦ä¸²ï¼ˆæœ€ç®€å•ï¼Œé€‚åˆå•æ¬¡é—®ç­”ï¼‰

**ä½¿ç”¨åœºæ™¯ï¼š** ç®€å•çš„ä¸€æ¬¡æ€§é—®ç­”ï¼Œä¸éœ€è¦è®¾ç½®ç³»ç»Ÿè§’è‰²æˆ–å¯¹è¯å†å²

**è¯­æ³•ï¼š**
```python
response = model.invoke("ä½ çš„é—®é¢˜æˆ–æŒ‡ä»¤")
```

**å®Œæ•´ç¤ºä¾‹ï¼š**
```python
from langchain.chat_models import init_chat_model

model = init_chat_model("groq:llama-3.3-70b-versatile", api_key="your_key")

# ç›´æ¥ä¼ é€’å­—ç¬¦ä¸²
response = model.invoke("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿç”¨ä¸€å¥è¯è§£é‡Š")

print(response.content)
# è¾“å‡ºï¼šæœºå™¨å­¦ä¹ æ˜¯ä¸€ç§è®©è®¡ç®—æœºé€šè¿‡æ•°æ®å­¦ä¹ è§„å¾‹ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹çš„æŠ€æœ¯ã€‚
```

**ä¼˜ç‚¹ï¼š**
- âœ… æœ€ç®€å•ï¼Œä»£ç æœ€å°‘
- âœ… é€‚åˆå¿«é€Ÿæµ‹è¯•

**ç¼ºç‚¹ï¼š**
- âŒ æ— æ³•è®¾ç½®ç³»ç»Ÿæç¤ºï¼ˆsystem promptï¼‰
- âŒ æ— æ³•ä¼ é€’å¯¹è¯å†å²
- âŒ çµæ´»æ€§è¾ƒä½

**ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ**
- å¿«é€Ÿæµ‹è¯•
- ç®€å•çš„ä¸€æ¬¡æ€§é—®ç­”
- ä¸éœ€è¦ä¸Šä¸‹æ–‡çš„åœºæ™¯

---

##### ğŸ“Œ æ ¼å¼ 2ï¼šå­—å…¸åˆ—è¡¨ï¼ˆæ¨èï¼Œæœ€çµæ´»ï¼‰

**ä½¿ç”¨åœºæ™¯ï¼š** éœ€è¦è®¾ç½®ç³»ç»Ÿè§’è‰²ã€å¤šè½®å¯¹è¯ã€ç²¾ç¡®æ§åˆ¶å¯¹è¯æµç¨‹

**è¯­æ³•ï¼š**
```python
messages = [
    {"role": "system", "content": "ç³»ç»Ÿæç¤º"},
    {"role": "user", "content": "ç”¨æˆ·æ¶ˆæ¯"},
    {"role": "assistant", "content": "AIå›å¤"},  # å¯é€‰ï¼Œç”¨äºå¯¹è¯å†å²
    {"role": "user", "content": "ç»§ç»­æé—®"}
]
response = model.invoke(messages)
```

**è§’è‰²è¯´æ˜ï¼š**

| è§’è‰² | è‹±æ–‡ | ä½œç”¨ | ç¤ºä¾‹ |
|------|------|------|------|
| `system` | System | è®¾å®š AI çš„è¡Œä¸ºã€è§’è‰²ã€è§„åˆ™ | "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python å¯¼å¸ˆ" |
| `user` | Human/User | ç”¨æˆ·çš„è¾“å…¥/é—®é¢˜ | "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ" |
| `assistant` | AI/Assistant | AI çš„å†å²å›å¤ï¼ˆç”¨äºå¯¹è¯ä¸Šä¸‹æ–‡ï¼‰ | "è£…é¥°å™¨æ˜¯ä¸€ç§è®¾è®¡æ¨¡å¼..." |

**å®Œæ•´ç¤ºä¾‹ 1ï¼šè®¾ç½®ç³»ç»Ÿæç¤º**
```python
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¼–ç¨‹å¯¼å¸ˆã€‚å›ç­”è¦ç®€æ´ã€å‡†ç¡®ï¼Œå¹¶æä¾›ä»£ç ç¤ºä¾‹ã€‚"
    },
    {
        "role": "user",
        "content": "ä»€ä¹ˆæ˜¯ Python åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"
    }
]

response = model.invoke(messages)
print(response.content)
```

**å®Œæ•´ç¤ºä¾‹ 2ï¼šå¤šè½®å¯¹è¯ï¼ˆå¸¦å†å²ï¼‰**
```python
# ç¬¬ä¸€è½®å¯¹è¯
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"},
    {"role": "user", "content": "æˆ‘å«å°æ˜"}
]

response1 = model.invoke(messages)
print(response1.content)  # "ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"

# ç¬¬äºŒè½®å¯¹è¯ - æ·»åŠ å†å²
messages.append({"role": "assistant", "content": response1.content})
messages.append({"role": "user", "content": "æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆï¼Ÿ"})

response2 = model.invoke(messages)
print(response2.content)  # "ä½ è¯´ä½ å«å°æ˜ã€‚"
```

**å®Œæ•´ç¤ºä¾‹ 3ï¼šæ„å»ºå®Œæ•´å¯¹è¯**
```python
# åˆå§‹åŒ–å¯¹è¯
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª Python ä¸“å®¶"}
]

# ç”¨æˆ·æé—® 1
conversation.append({"role": "user", "content": "ä»€ä¹ˆæ˜¯åˆ—è¡¨ï¼Ÿ"})
response1 = model.invoke(conversation)
print(f"AI: {response1.content}")

# ä¿å­˜ AI å›å¤åˆ°å†å²
conversation.append({"role": "assistant", "content": response1.content})

# ç”¨æˆ·æé—® 2ï¼ˆåŸºäºä¸Šä¸‹æ–‡ï¼‰
conversation.append({"role": "user", "content": "å®ƒå’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"})
response2 = model.invoke(conversation)
print(f"AI: {response2.content}")

# æ­¤æ—¶ conversation åŒ…å«å®Œæ•´çš„å¯¹è¯å†å²
print(f"\nå®Œæ•´å¯¹è¯å†å²: {conversation}")
```

**ä¼˜ç‚¹ï¼š**
- âœ… æœ€çµæ´»ï¼Œå®Œå…¨æ§åˆ¶
- âœ… å¯ä»¥è®¾ç½®ç³»ç»Ÿæç¤º
- âœ… æ”¯æŒå¤šè½®å¯¹è¯
- âœ… ä¸ OpenAI API æ ¼å¼ä¸€è‡´
- âœ… JSON å…¼å®¹ï¼Œæ˜“äºå­˜å‚¨å’Œä¼ è¾“

**ç¼ºç‚¹ï¼š**
- âŒ ä»£ç ç¨å¾®å¤šä¸€ç‚¹ï¼ˆä½†æ›´æ¸…æ™°ï¼‰

**ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ**
- âœ… **æ¨èç”¨äºæ‰€æœ‰åœºæ™¯**
- éœ€è¦è®¾ç½®ç³»ç»Ÿè§’è‰²
- å¤šè½®å¯¹è¯
- éœ€è¦ä¿å­˜å¯¹è¯å†å²
- ç”Ÿäº§ç¯å¢ƒåº”ç”¨

---

##### ğŸ“Œ æ ¼å¼ 3ï¼šæ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼ˆç±»å‹å®‰å…¨ï¼Œä½†è¾ƒç¹çï¼‰

**ä½¿ç”¨åœºæ™¯ï¼š** éœ€è¦ç±»å‹æ£€æŸ¥ã€IDE è‡ªåŠ¨è¡¥å…¨çš„åœºæ™¯

**è¯­æ³•ï¼š**
```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage(content="ç³»ç»Ÿæç¤º"),
    HumanMessage(content="ç”¨æˆ·æ¶ˆæ¯"),
    AIMessage(content="AIå›å¤")
]
response = model.invoke(messages)
```

**æ¶ˆæ¯ç±»å‹å¯¹ç…§ï¼š**

| æ¶ˆæ¯ç±» | å¯¹åº”å­—å…¸æ ¼å¼ | ä½œç”¨ |
|--------|-------------|------|
| `SystemMessage` | `{"role": "system", ...}` | ç³»ç»Ÿæç¤º |
| `HumanMessage` | `{"role": "user", ...}` | ç”¨æˆ·è¾“å…¥ |
| `AIMessage` | `{"role": "assistant", ...}` | AI å›å¤ |

**å®Œæ•´ç¤ºä¾‹ï¼š**
```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ª Python ä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯ç”Ÿæˆå™¨ï¼Ÿ"),
]

response = model.invoke(messages)

# ç»§ç»­å¯¹è¯
messages.append(AIMessage(content=response.content))
messages.append(HumanMessage(content="èƒ½ç»™ä¸ªä¾‹å­å—ï¼Ÿ"))

response2 = model.invoke(messages)
```

**ä¼˜ç‚¹ï¼š**
- âœ… ç±»å‹å®‰å…¨
- âœ… IDE è‡ªåŠ¨è¡¥å…¨
- âœ… æ›´å®¹æ˜“å‘ç°é”™è¯¯

**ç¼ºç‚¹ï¼š**
- âŒ ä»£ç è¾ƒé•¿
- âŒ ä¸å¦‚å­—å…¸ç®€æ´
- âŒ éš¾ä»¥åºåˆ—åŒ–ï¼ˆJSONï¼‰

**ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ**
- å¤§å‹é¡¹ç›®ï¼Œéœ€è¦ç±»å‹æ£€æŸ¥
- å›¢é˜Ÿåä½œï¼Œéœ€è¦ä¸¥æ ¼è§„èŒƒ
- ä½¿ç”¨ TypeScript/MyPy ç­‰ç±»å‹æ£€æŸ¥å·¥å…·

---

#### ğŸ invoke è¿”å›å€¼è¯¦è§£

`invoke` è¿”å›ä¸€ä¸ª **AIMessage å¯¹è±¡**ï¼ŒåŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼š

**è¿”å›å¯¹è±¡ç»“æ„ï¼š**
```python
response = model.invoke("Hello")

# 1. ä¸»è¦å†…å®¹
response.content              # str - AI çš„å›å¤æ–‡æœ¬
response.response_metadata    # dict - å“åº”å…ƒæ•°æ®
response.id                   # str - æ¶ˆæ¯å”¯ä¸€ ID
response.usage_metadata       # dict - Token ä½¿ç”¨æƒ…å†µ
response.additional_kwargs    # dict - å…¶ä»–é¢å¤–ä¿¡æ¯
```

**å®Œæ•´ç¤ºä¾‹ï¼šè®¿é—®æ‰€æœ‰ä¿¡æ¯**
```python
response = model.invoke("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯ AI")

# 1. è·å–å›å¤å†…å®¹
print("AI å›å¤:", response.content)

# 2. è·å–æ¨¡å‹ä¿¡æ¯
metadata = response.response_metadata
print(f"ä½¿ç”¨çš„æ¨¡å‹: {metadata['model_name']}")
print(f"ç»“æŸåŸå› : {metadata['finish_reason']}")

# 3. è·å– Token ä½¿ç”¨æƒ…å†µ
usage = metadata.get('token_usage', {})
print(f"æç¤º tokens: {usage.get('prompt_tokens')}")
print(f"å®Œæˆ tokens: {usage.get('completion_tokens')}")
print(f"æ€»è®¡ tokens: {usage.get('total_tokens')}")

# 4. è·å–æ¶ˆæ¯ ID
print(f"æ¶ˆæ¯ ID: {response.id}")
```

**response_metadata å®Œæ•´ç»“æ„ï¼š**
```python
{
    'model_name': 'llama-3.3-70b-versatile',      # ä½¿ç”¨çš„æ¨¡å‹
    'system_fingerprint': 'fp_4cfc2deea6',        # ç³»ç»ŸæŒ‡çº¹
    'finish_reason': 'stop',                      # ç»“æŸåŸå› ï¼šstop/length/error
    'model_provider': 'groq',                     # æ¨¡å‹æä¾›å•†
    'token_usage': {                              # Token ä½¿ç”¨ç»Ÿè®¡
        'prompt_tokens': 15,                      # è¾“å…¥ tokens
        'completion_tokens': 25,                  # è¾“å‡º tokens
        'total_tokens': 40,                       # æ€»è®¡ tokens
        'prompt_time': 0.002,                     # è¾“å…¥å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        'completion_time': 0.23                   # è¾“å‡ºç”Ÿæˆæ—¶é—´ï¼ˆç§’ï¼‰
    }
}
```

---

#### ğŸ”§ config å‚æ•°ï¼ˆé«˜çº§ç”¨æ³•ï¼‰

`config` å‚æ•°ç”¨äºä¼ é€’é«˜çº§é…ç½®ï¼Œä¸€èˆ¬åˆå­¦è€…ä¸éœ€è¦ç”¨åˆ°ã€‚

**å¸¸ç”¨é…ç½®ï¼š**

```python
config = {
    "callbacks": [callback_handler],      # å›è°ƒå‡½æ•°
    "tags": ["test", "development"],      # æ ‡ç­¾ï¼ˆç”¨äºè¿½è¸ªï¼‰
    "metadata": {"user_id": "123"},       # å…ƒæ•°æ®
    "run_name": "my_query"                # è¿è¡Œåç§°
}

response = model.invoke(messages, config=config)
```

**æš‚æ—¶å¯ä»¥å¿½ç•¥ï¼Œåç»­ä¼šè¯¦ç»†å­¦ä¹ ã€‚**

---

#### ğŸ“š å®æˆ˜ç¤ºä¾‹æ±‡æ€»

**ç¤ºä¾‹ 1ï¼šæœ€ç®€å•çš„é—®ç­”**
```python
response = model.invoke("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
print(response.content)
```

**ç¤ºä¾‹ 2ï¼šå¸¦ç³»ç»Ÿæç¤ºçš„é—®ç­”**
```python
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜çš„åŠ©æ‰‹ï¼Œå–œæ¬¢ç”¨æ¯”å–»è§£é‡Šæ¦‚å¿µ"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ"}
]
response = model.invoke(messages)
print(response.content)
```

**ç¤ºä¾‹ 3ï¼šå¤šè½®å¯¹è¯**
```python
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹"}
]

# ç¬¬ä¸€è½®
conversation.append({"role": "user", "content": "Python ä¸­å¦‚ä½•å®šä¹‰å‡½æ•°ï¼Ÿ"})
r1 = model.invoke(conversation)
conversation.append({"role": "assistant", "content": r1.content})

# ç¬¬äºŒè½®
conversation.append({"role": "user", "content": "é‚£å‚æ•°æ€ä¹ˆä¼ é€’ï¼Ÿ"})
r2 = model.invoke(conversation)
print(r2.content)
```

**ç¤ºä¾‹ 4ï¼šç›‘æ§ Token ä½¿ç”¨**
```python
response = model.invoke("å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—")
usage = response.response_metadata['token_usage']

print(f"æœ¬æ¬¡è°ƒç”¨ä½¿ç”¨äº† {usage['total_tokens']} ä¸ª tokens")
print(f"æˆæœ¬é¢„ä¼°: ${usage['total_tokens'] * 0.0001:.4f}")  # å‡è®¾æ¯åƒtokens $0.1
```

---

### 3. Messages - æ¶ˆæ¯ç±»å‹

LangChain ä½¿ç”¨ä¸åŒçš„æ¶ˆæ¯ç±»å‹æ¥è¡¨ç¤ºå¯¹è¯ä¸­çš„ä¸åŒè§’è‰²ã€‚

#### æ¶ˆæ¯ç±»å‹æ¦‚è§ˆ

| æ¶ˆæ¯ç±»å‹ | è§’è‰² | ç”¨é€” | ç¤ºä¾‹ |
|---------|------|------|------|
| `SystemMessage` | `system` | è®¾å®š AI çš„è¡Œä¸ºã€è§’è‰²ã€è§„åˆ™ | "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°å­¦è€å¸ˆ" |
| `HumanMessage` | `user` | ç”¨æˆ·çš„è¾“å…¥ | "ä»€ä¹ˆæ˜¯å¾®ç§¯åˆ†ï¼Ÿ" |
| `AIMessage` | `assistant` | AI çš„å›å¤ | "å¾®ç§¯åˆ†æ˜¯ç ”ç©¶å˜åŒ–ç‡çš„æ•°å­¦åˆ†æ”¯..." |

#### ä½¿ç”¨æ¶ˆæ¯å¯¹è±¡

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# åˆ›å»ºæ¶ˆæ¯
system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹")
human_msg = HumanMessage(content="ä½ å¥½")
ai_msg = AIMessage(content="ä½ å¥½ï¼æˆ‘èƒ½å¸®ä½ ä»€ä¹ˆï¼Ÿ")

# æ„å»ºå¯¹è¯å†å²
messages = [system_msg, human_msg, ai_msg]
messages.append(HumanMessage(content="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"))

# è°ƒç”¨æ¨¡å‹
response = model.invoke(messages)
```

#### ä½¿ç”¨å­—å…¸æ ¼å¼ï¼ˆæ¨èï¼‰

```python
# æ›´ç®€æ´çš„å­—å…¸æ ¼å¼
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘èƒ½å¸®ä½ ä»€ä¹ˆï¼Ÿ"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
]

response = model.invoke(messages)
```

#### ä¸ºä»€ä¹ˆä½¿ç”¨ä¸åŒçš„æ¶ˆæ¯ç±»å‹ï¼Ÿ

1. **æ˜ç¡®è§’è‰²**ï¼šæ¸…æ™°åŒºåˆ†ç³»ç»Ÿæç¤ºã€ç”¨æˆ·è¾“å…¥å’Œ AI å›å¤
2. **å¯¹è¯å†å²**ï¼šæ„å»ºå®Œæ•´çš„å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
3. **æ§åˆ¶è¡Œä¸º**ï¼šé€šè¿‡ SystemMessage ç²¾ç¡®æ§åˆ¶ AI çš„è¡Œä¸º
4. **è°ƒè¯•å‹å¥½**ï¼šæ›´å®¹æ˜“è¿½è¸ªå’Œè°ƒè¯•å¯¹è¯æµç¨‹

---

## å®Œæ•´ç¤ºä¾‹ä»£ç è¯´æ˜

`main.py` æ–‡ä»¶åŒ…å« 7 ä¸ªæ¸è¿›å¼ç¤ºä¾‹ï¼š

### ç¤ºä¾‹ 1ï¼šæœ€ç®€å•çš„ LLM è°ƒç”¨
- æ¼”ç¤ºåŸºæœ¬çš„ `init_chat_model` å’Œ `invoke` ä½¿ç”¨
- ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥

### ç¤ºä¾‹ 2ï¼šä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨è¿›è¡Œå¯¹è¯
- ä½¿ç”¨ `SystemMessage` å’Œ `HumanMessage`
- æ„å»ºå¤šè½®å¯¹è¯å†å²

### ç¤ºä¾‹ 3ï¼šä½¿ç”¨å­—å…¸æ ¼å¼çš„æ¶ˆæ¯ï¼ˆæ¨èï¼‰
- æ›´ç®€æ´çš„å­—å…¸æ ¼å¼
- ä¸ OpenAI API æ ¼å¼ä¸€è‡´

### ç¤ºä¾‹ 4ï¼šé…ç½®æ¨¡å‹å‚æ•°
- å¯¹æ¯”ä¸åŒ `temperature` çš„æ•ˆæœ
- ä½¿ç”¨ `max_tokens` é™åˆ¶è¾“å‡º

### ç¤ºä¾‹ 5ï¼šç†è§£ invoke è¿”å›å€¼
- è¯¦ç»†è§£æ `AIMessage` å¯¹è±¡
- è®¿é—®å…ƒæ•°æ®å’Œ token ä½¿ç”¨æƒ…å†µ

### ç¤ºä¾‹ 6ï¼šé”™è¯¯å¤„ç†
- æ•è·å’Œå¤„ç†å¸¸è§é”™è¯¯
- ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µ

### ç¤ºä¾‹ 7ï¼šå¤šæ¨¡å‹å¯¹æ¯”
- è½»æ¾åˆ‡æ¢ä¸åŒæ¨¡å‹
- å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¾“å‡º

---

## ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
pip install langchain langchain-groq python-dotenv
```

### 2. é…ç½® API å¯†é’¥

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
GROQ_API_KEY=your_groq_api_key_here
```

### 3. è·å– Groq API å¯†é’¥

1. è®¿é—® [Groq Console](https://console.groq.com/)
2. æ³¨å†Œè´¦å·
3. åœ¨ API Keys é¡µé¢åˆ›å»ºæ–°çš„ API å¯†é’¥
4. å¤åˆ¶å¯†é’¥åˆ° `.env` æ–‡ä»¶

---

## è¿è¡Œç¤ºä¾‹

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
cd phase1_fundamentals/01_hello_langchain

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
python main.py
```

---

## å¸¸è§é—®é¢˜ (FAQ)

### Q1: init_chat_model å’Œç›´æ¥ä½¿ç”¨ ChatGroq æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A:** `init_chat_model` æ˜¯ LangChain 1.0 çš„ç»Ÿä¸€æ¥å£ï¼Œä¼˜åŠ¿åŒ…æ‹¬ï¼š
- è·¨æ¨¡å‹æä¾›å•†çš„ä¸€è‡´ API
- æ›´ç®€æ´çš„è¯­æ³•
- æ›´å¥½çš„ç±»å‹æç¤º
- æ›´å®¹æ˜“åˆ‡æ¢æ¨¡å‹

```python
# æ—§æ–¹å¼ï¼ˆä»ç„¶å¯ç”¨ï¼‰
from langchain_groq import ChatGroq
model = ChatGroq(model="llama-3.3-70b-versatile", api_key="...")

# æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
from langchain.chat_models import init_chat_model
model = init_chat_model("groq:llama-3.3-70b-versatile", api_key="...")
```

### Q2: temperature å‚æ•°å¦‚ä½•é€‰æ‹©ï¼Ÿ

**A:** æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©ï¼š
- **0.0-0.3**ï¼šéœ€è¦ä¸€è‡´æ€§ã€å‡†ç¡®æ€§çš„ä»»åŠ¡ï¼ˆæ•°æ®æå–ã€åˆ†ç±»ã€ä»£ç ç”Ÿæˆï¼‰
- **0.5-0.7**ï¼šå¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§ï¼ˆèŠå¤©ã€é—®ç­”ï¼‰
- **0.8-1.5**ï¼šåˆ›é€ æ€§ä»»åŠ¡ï¼ˆå†™ä½œã€å¤´è„‘é£æš´ï¼‰
- **1.5-2.0**ï¼šé«˜åº¦åˆ›é€ æ€§ï¼ˆè¯—æ­Œã€æ•…äº‹åˆ›ä½œï¼‰

### Q3: invoke å’Œ stream æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A:**
- `invoke`ï¼šåŒæ­¥è°ƒç”¨ï¼Œç­‰å¾…å®Œæ•´å“åº”åè¿”å›
- `stream`ï¼šæµå¼è°ƒç”¨ï¼Œå®æ—¶è¿”å›å“åº”ç‰‡æ®µï¼ˆæˆ‘ä»¬å°†åœ¨åç»­æ¨¡å—å­¦ä¹ ï¼‰

```python
# invoke - ç­‰å¾…å®Œæ•´å“åº”
response = model.invoke("å†™ä¸€é¦–è¯—")
print(response.content)  # ä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´è¯—æ­Œ

# stream - å®æ—¶æµå¼è¾“å‡ºï¼ˆåç»­å­¦ä¹ ï¼‰
for chunk in model.stream("å†™ä¸€é¦–è¯—"):
    print(chunk.content, end="", flush=True)  # é€å­—è¾“å‡º
```

### Q4: ä¸ºä»€ä¹ˆæ¨èä½¿ç”¨å­—å…¸æ ¼å¼è€Œä¸æ˜¯æ¶ˆæ¯å¯¹è±¡ï¼Ÿ

**A:** ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥ï¼Œä½†å­—å…¸æ ¼å¼æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- æ›´ç®€æ´ï¼Œä»£ç é‡æ›´å°‘
- ä¸ OpenAI API æ ¼å¼ä¸€è‡´
- æ›´å®¹æ˜“åºåˆ—åŒ–å’Œå­˜å‚¨
- JSON å…¼å®¹ï¼Œä¾¿äºç½‘ç»œä¼ è¾“

### Q5: å¦‚ä½•å¤„ç† API è°ƒç”¨å¤±è´¥ï¼Ÿ

**A:** ä½¿ç”¨ try-except å—æ•è·å¼‚å¸¸ï¼š

```python
try:
    response = model.invoke("Hello")
    print(response.content)
except ValueError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
except ConnectionError as e:
    print(f"ç½‘ç»œé”™è¯¯: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

---

## æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç† API å¯†é’¥

```python
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("GROQ_API_KEY")
```

### 2. éªŒè¯ API å¯†é’¥å­˜åœ¨

```python
if not os.getenv("GROQ_API_KEY"):
    raise ValueError("è¯·è®¾ç½® GROQ_API_KEY ç¯å¢ƒå˜é‡!")
```

### 3. ä½¿ç”¨ SystemMessage æ§åˆ¶è¡Œä¸º

```python
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”ç®€æ´çš„åŠ©æ‰‹ã€‚å›ç­”é™åˆ¶åœ¨100å­—ä»¥å†…ã€‚"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}
]
```

### 4. ä¿å­˜å’Œé‡ç”¨å¯¹è¯å†å²

```python
# åˆå§‹åŒ–å¯¹è¯å†å²
conversation = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"}
]

# æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
conversation.append({"role": "user", "content": "ä½ å¥½"})

# è°ƒç”¨æ¨¡å‹
response = model.invoke(conversation)

# ä¿å­˜ AI å›å¤åˆ°å†å²
conversation.append({"role": "assistant", "content": response.content})

# ç»§ç»­å¯¹è¯
conversation.append({"role": "user", "content": "æˆ‘æƒ³å­¦ Python"})
response = model.invoke(conversation)
```

### 5. ç›‘æ§ Token ä½¿ç”¨

```python
response = model.invoke("Hello")
usage = response.response_metadata.get("token_usage", {})
print(f"Token ä½¿ç”¨: {usage.get('total_tokens', 'N/A')}")
```

---

## LangChain 1.0 é‡è¦å˜æ›´

### ä» 0.x åˆ° 1.0 çš„ä¸»è¦å˜åŒ–

1. **ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–**
   - æ—§ï¼šä½¿ç”¨ç‰¹å®šçš„ç±»ï¼ˆå¦‚ `ChatGroq`, `ChatOpenAI`ï¼‰
   - æ–°ï¼šä½¿ç”¨ `init_chat_model` ç»Ÿä¸€æ¥å£

2. **ç®€åŒ–çš„ Agent åˆ›å»º**
   - æ—§ï¼šä½¿ç”¨ `create_react_agent` ç­‰å¤šä¸ªå‡½æ•°
   - æ–°ï¼šä½¿ç”¨ `create_agent` ç»Ÿä¸€æ¥å£ï¼ˆæˆ‘ä»¬å°†åœ¨æ¨¡å— 5 å­¦ä¹ ï¼‰

3. **LangGraph ä½œä¸ºè¿è¡Œæ—¶**
   - LangChain 1.0 æ„å»ºåœ¨ LangGraph ä¹‹ä¸Š
   - æ›´å¼ºå¤§çš„çŠ¶æ€ç®¡ç†å’Œå·¥ä½œæµæ§åˆ¶

4. **ä¸­é—´ä»¶ç³»ç»Ÿ**
   - æ–°å¢ä¸­é—´ä»¶æ¶æ„ï¼ˆæˆ‘ä»¬å°†åœ¨æ¨¡å— 10-12 å­¦ä¹ ï¼‰
   - æ›´å¥½çš„å¯è§‚æµ‹æ€§å’Œæ§åˆ¶æµ

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

å®Œæˆæœ¬æ¨¡å—åï¼Œç»§ç»­å­¦ä¹ ï¼š

1. **02_prompt_templates** - å­¦ä¹ æç¤ºè¯æ¨¡æ¿ï¼Œé¿å…å­—ç¬¦ä¸²æ‹¼æ¥
2. **03_messages** - æ·±å…¥ç†è§£æ¶ˆæ¯ç±»å‹å’Œå¯¹è¯ç®¡ç†
3. **04_custom_tools** - åˆ›å»ºè‡ªå®šä¹‰å·¥å…·
4. **05_simple_agent** - ä½¿ç”¨ `create_agent` æ„å»ºç¬¬ä¸€ä¸ª Agent

---

## å‚è€ƒèµ„æº

- [LangChain 1.0 å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/quickstart)
- [LangChain 1.0 è¿ç§»æŒ‡å—](https://docs.langchain.com/oss/python/migrate/langchain-v1)
- [Groq æ–‡æ¡£](https://console.groq.com/docs)
- [LangChain API å‚è€ƒ](https://docs.langchain.com/oss/python/api_reference/)

---

## å°ç»“

é€šè¿‡æœ¬æ¨¡å—ï¼Œä½ å·²ç»å­¦ä¹ äº†ï¼š

- âœ… å¦‚ä½•ä½¿ç”¨ `init_chat_model` åˆå§‹åŒ–èŠå¤©æ¨¡å‹
- âœ… å¦‚ä½•ä½¿ç”¨ `invoke` æ–¹æ³•è°ƒç”¨æ¨¡å‹
- âœ… ç†è§£ä¸åŒçš„æ¶ˆæ¯ç±»å‹ï¼ˆSystem, Human, AIï¼‰
- âœ… é…ç½®æ¨¡å‹å‚æ•°ï¼ˆtemperature, max_tokensï¼‰
- âœ… å¤„ç†å“åº”å’Œå…ƒæ•°æ®
- âœ… é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

**æ­å–œï¼ä½ å·²ç»è¿ˆå‡ºäº† LangChain 1.0 å­¦ä¹ çš„ç¬¬ä¸€æ­¥ï¼** ğŸ‰
